\documentclass[12,twoside]{TFG-GM}
%\usepackage[active]{srcltx}
\usepackage{amsthm,amsmath,amssymb,amsfonts,amscd}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[all]{xy}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{url}
%\usepackage[usenames]{xcolor}
%\usepackage{fancyhdr}

%%%%%Author packages if necessary

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Theorem Environments: add extra ones at the end if you need it.

\newtheorem*{theoremA}{Theorem A}
\newtheorem{theorem}{Theorem}[section]

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remarknonumber}{Remark}
\newtheorem{observation}[theorem]{Observation}




%%%%%%%%%%%%%%%%%%
% macros/abbreviations: Include here your own.
%%%%%%%%%%%%%%%%%%

\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand*\mean[1]{\bar{#1}}
\newcommand*\diff[1]{\bar{#1}}


% Body of document

\titol{A Learning Approach\\[3mm] To The FOM Problem}
\titolcurt{A learning approach to the FOM problem}
\authorStudent{Eudald Romo Grau}
\supervisors{Alberto Rodriguez Garcia and Maria Alberich Carrami\~nana}
\monthYear{April, 2017}

%\msc[2010]{Primary  	55M25, 57P10, Secondary 55P15, 57R19, 57N15.}

\paraulesclau{Manipulation, Online Control, MPC, FOM, Underactuation, Hybridness}
\agraiments{
Thanks to Alberto Rodriguez for his tutoring, for providing me with the required tools and financial support to undertake this project and for hosting me in his laboratory, to Maria Alberich for her supervision and tutoring, to Francois Hogan for his ideas and the introduction he gave me to his previous work, to Maria Bauza for her insights in Machine Learning, to all the members of the MCube Lab for their thoughts on the project and their support, to Centre de Formacio Interdisciplinaria Superior for offering me the possibility and the financial support to take part in this project, to Massachusetts Institute of Technology for providing the required facilities required to develop this project and to Generalitat de Catalunya for their financial support.}


\abstracteng{The family of modes (FOM) approach to solving model predictive control (MPC) problems is a novel heuristic technique developed at MIT to solve the time complexity of traditional MPC solving techniques. This study addresses some of the issues associated with the previous formulations of this technique by increasing the sequential robustness of the FOM and providing methodologies to choose the parameters required for the heuristic. A general simulation interface is developed together with techniques to score and compare the obtained trajectories. Then an statistics and machine learning based methodology is developed to tune the parameters is proposed and the results are compared with the original ones. Finally, experimental procedures are developed to validate the results.}

%%%%%%%%%
\begin{document}

\maketitle

\section{Goal of this study}
\label{briefgoal}
Hogan \cite{fom} recently provided an heuristic technique called family of modes (FOM) to solve model predictive control (MPC) problems under hybrid constraints and underactuation. The goal of this study will be revisited in section \ref{goal} but the main objective of this study was to expand this new method usage in the robotics manipulation community.

In order to do that, we wanted to reinforce the method by fixing some of it's weaknesses (as it's sequential properties), to provide comparison tools to test the method against traditional hybrid MPC solving techniques as mixed integer quadratic programming (MIQP), and to provide simple and systematic techniques to optimize a general MPC problem.

To proof the scalability of this method we wanted to increase the complexity of the systems beyond the planar problem of a single point pusher and a square slider used in Hogan's work.

Some simple control policies as PID controllers are able to control systems that scape the model they were designed for. We decided to check the robustness of the FOM method by using it in models that were different from those it was designed for.

We hope that this work will increase the performance of FOM and bring it closer to the robotics manipulation research community by both providing systematic set-up tools and showing it's robustness and scalability.

\section{Introduction. On the mechanics of pushing.}
\label{sec:intro}

Traditionally, one of the main focus of interest of the manipulation community has been the mechanics of stable grasping: finding stable grasping points, encapsulation, etc. [add citations and some examples, Alberto's work, M. Mason, etc.]. But some drift towards the study of the mechanics of pushing started to appear at least as far as Mason's work on 1980's \cite{pushing4}. Some of the motivations for this new focus of interest were to allow the proper understanding of the internal mechanics of general grasping (by viewing it as a special case of multi-contact-point pushing) to {adsfa} and to address some of the issues found with stable grasping [TODO: Add some of the things mentioned in Mason(1986): Humans example, multiple objects handling, ...].

Goyal et al. \cite{planar_sliding1}\cite{planar_sliding2} applied concepts of classical plastic theory to describe planar sliding, deriving the concepts of limit surfaces for dry contact manipulation.

Planar pushing has been widely studied since, both by its simpler model as compared to free 3D pushing as for it's similarity to simple in-hand repositioning operations [TODO: add citation].

%In Mason's study of the mechanics of pushing [TODO: Ask Maria if this is proper] there's an introduction to the traditional assumptions and previous results on the mechanics of pushing, which I'm going to borrow in this thesis.

\subsection{Coulomb Friction}
\label{subsec:coulomb}
One of the usual assumptions in the robotics community is considering dry Coulomb contact forces. This model separates friction into two possible states (static and dynamic or kinetic friction) and the set of rules which describe it are a combination the work of Amonton [TODO: cite] and Coulomb [TODO: cite]:
\begin{itemize}
\item{Amonton's first law:} The force of friction is directly proportional to the applied load.
\item{Amonton's second law:} The force of friction is independent of the apparent area of contact.
\item{Coulomb's law:} Kinetic friction is independent of the sliding velocity.
\end{itemize}
Mason cites prior statements by Da Vinci[TODO: Cite] and experimental verification by Truesdell and Guillmor [TODO: Cite] and I would like to add the work of Euler[TODO: Cite], who first distinguished between static and kinetic friction.

\subsection{Friction Cone}
\label{subsec:frictioncone}
A common and useful geometric interpretation of Coulomb’s law (according to Mason first constructed by Moseley[TODO: Cite]) is widely used in the robotics community. Given a point on a surface interacting with it with total contact force \textbf{f}, we decompose it into it's normal $f_n$ and tangential $f_t$ components with respect to the contact surface. Coulomb friction compact formulation states that $f_t \leq \mu f_n$, where $\mu$ is the proportionality factor usually called friction coefficient [TODO: Add note on why only one coefficient is used], where the equality holds on dynamic friction. Letting $\alpha$ be the angle between \textbf{f} and $f_n$ (or, alternatively, the normal of the surface on the point of contact), Coulomb's law is equivalent to 
If we construct the normal to the surface, then Coulomb’s law is equivalent to $\alpha \leq tan^{-1} \frac{f_t}{f_n}$. So, the set of feasible contact forces lies inside a closed cone with aperture $2\alpha$. [TODO: Add image]

\subsection{Maximum Power Inequality, Limit Curve and Limit Surface}
\label{subsec:mpi}
Goyal's work can be generalised to friction models which are more general but, for clarity, the main concepts will be introduced with the special case of isotropic Coulomb friction (as was presented in their original paper).

Given rigid body sliding over a surface with a known normal force (or pressure distribution) and assuming that, at each point of contact, the frictional force depends only on the normal force, the slider's orientation and it's direction of slipping, isotropic Coulomb friction law allows us to obtain two important properties.
\begin{enumerate}
\item $|f| \leq K$, where f is the frictional force vector and K is a constant. Furthermore,
\end{enumerate}

The maximum-power inequality is introduced with which a convex limit curve describes the forces arising during slip of a point of contact. The limit curves for Coulomb friction (a circle), for an ideal wheel (a straight line), and for some less ideal wheels are given as examples. The load-motion inequality for the overall body is then derived and the resulting concept of a limit surface is introduced and illustrated with two somewhat artificial examples (a body with two points of Coulombic support, and a ring of ratcheted wheels). The moment function is then presented. We conclude with a discussion of some facts and results related to limit surfaces and to the moment function.

\subsection{Motion Cone and Limit Surfaces}
\label{subsec:motioncone}

\subsection{Quasi static formulation}
\label{subsec:quasistatic}

\subsection{Data driven approach}
\label{subsec:datadriven} 

Work on how to link it with previous things (maybe some paper on problems of the model driven approach)

\subsection{Hybridness}
\label{subsec:hybridness}

The study of the mechanics of pushing led to the concept of hybridness, inherent in the dynamical properties of traditional rigid body Coulomb contact  

Feedback is hard/requires effort. All the teams in the ARC used open loop systems (as of the 2nd edition of the competition) [add reference]

Feedback is important. Robots fall because control in hybrid systems still has a long way to go.

TODO: UNDERACTUATION


This allowed to tackle problems as state hybridness (ADD COMPLEMENTARITY CONSTRAINTS AND HYBRID PROBLEMS). This allowed further progress into the planning branch of these problems. Unluckily, the complexity of the traditional formulations for these problems requires discretization of the time space of the problem and the algorithmic cost of the used solvers usually grows exponentially with the number of considered time steps. This has been translated into a relatively lower progress of the correspondent online control branch.

\section{Prior Work}
\label{sec:priorWork}

\subsection{Pushing}
\label{subsec:pushing}



\subsection{Complementarity Constraints}
\label{subsec:Complementarity Constraint}
[TODO: Read Nima's paper].

The Complementarity constraint formulation [TODO: INSERT REFERENCES] presented a systematic way of formulating hybrid system problems and the work of Anitescu and Potra \cite{lcc1} guaranteed a solution for this formulation on multi-rigid-body Coulomb friction contact problems and impact problems with friction and nonzero restitution coefficient. However, the complementarity constraints are generally non convex and using them in practical problems without further treatment makes the problem non-computable.

\subsection{Model Predictive Control}
\label{subsec:MPC}
To tackle the computing cost of complementarity constraints, model predictive control(MPC) is usually used. This formulation defines a discrete set of positions $(x_1, ..., x_n)$ and control inputs $(u_0, ..., u_{n-1})$ and assumes that the hybrid state the system is in time $t_i$ cannot change until time $t_{i+1}$. Given convex dynamics and cost function and fixing the state the system is during each time transition, the optimization problem on the control inputs is convex [TODO: Citation]. Thus, the global problem can be solved by addressing each of the convex fixed hybrid state sub-problems.
As the number of sub-problems grows exponentially with the number of time steps, a mixed integer programming (MIP) strategy is usually adopted. The drawbacks of MIP is that it doesn't escalate well with the number of possible hybrid states and, even with a small number of states, the optimizers aren't generally fast enough to solve the problem online, as required in control problems.

[TODO: Add this: Es pot representar el conjunt de possibles problemes de programacio lineal que s'han de resoldre a traves d'un arbre. Cada node representa un dels tres estats possibles. L'arrel de l'arbre es un node buit i a cada pas de la trajectoria l'arbre es ramifica 3 nodes (un per cada possible estat). Cada cami des de l'arrel fins a una de les fulles defineix de manera unica un problema a solucionar.]

\subsection{MIP and MIQP}
\label{subsec:intro_miqp}

\subsection{Family of Modes}
\label{subsec:fom}
Recently, Hogan and Rodriguez \cite{fom} proposed an heuristic based procedure to solve MPC problems online called Family of Modes (FOM). In their work, they assume a finite horizon MPC problem. Periodically, the heuristic method explores the optimization problem over only a small set of all the possible combinations of hybrid states to decide the instantaneous controller policy. This policy is then recomputed frequently to take into account new information as the finite horizon advances in time as well as to tackle external interference with the system and errors in the dynamic model.

FOM is applied to solve a planar system formed by a point pusher and a square slider supported on a board. The contact between the pusher and the slider is modelled as a single point Coulomb contact. Three possible hybrid states are considered with regards to this contact point: pushing without sliding and pushing while sliding in each of the two possible directions. The contact between the slider and the board is modelled by deriving the limit surface of the slider [TODO: Citation]. The motion and cone constraints are obtained by assuming a quasi-static formulation and linearising the motion equations and the motion cone [TODO: explain more and explain why we use full dynamics on the first step]. By exploring only three pre-chosen modes at every step their controller is able to successfully track a straight trajectory or pass through a set of waypoints even under external perturbations or perturbations on the dynamical model used. [TODO: add this:(per exemple, podriem estar esperant que en un pas el dit robotic es mantingues adherit, pero el coeficient de friccio pot ser diferent a la zona on esta en aquell moment i lliscar en comptes de mantenir-se adherit)]

However, the method depends on the chosen modes to explore. In this simple case a good set of modes could be derived intuitively or by trial and error, but for more complex and higher dimensional problems, this may not be possible. Furthermore, adding more modes to consider into this heuristic doesn't generally translate into better results and, in some cases, it may lead to results far worse than the original one. So even guessing a good set of modes doesn't provide an easy or systematic way of improving the obtained result.

In this study the FOM heuristic is reinforced to provide non-worsening local solutions to the optimization problem as new modes are added into the family and a systematic way of choosing the initial modes. Finally, a generalized method is proposed to track complex trajectories. [TODO: Explain better].



\subsection{Nomenclature}
\label{subsec:nomenclature}
From here on:
\begin{enumerate}[\bf (1)]
\item{\tt hybrid state or state:} Each of the possible dynamic states the system can be in. [TODO: further define]
\item{\tt hybrid mode or mode:} Ordered set of states. It defines a finite horizon convex optimization program.
\item{\tt family of modes or family:} Unordered set of modes. Corresponds to each of the convex problems explored at each iteration of the FOM heuristic.
[TODO: Formally define to avoid any kind of ambiguities]
\end{enumerate}

\subsection{Sequential Properties}
\label{subsec:sequential}
Let's cite \cite{seq}

\section{Goal Revisited}
\label{goal}

To solve MPC problems fast enough to implement them online in a control problem is an open problem of interest for the robotics manipulation community, as explained in (TODO: Write it and cite it). We consider that Hogan's work in the FOM technique is a good way to provide an heuristic-based approximate solution to this problem. The main objective of this study is to bring this method close to the research community by:
\begin{enumerate}
\item {\textbf{Improving the properties of FOM:}} In section \ref{subsec:chameleon} we will discuss some of it's sequential weaknesses and the solutions we provided and in section \ref{subsec:dynfom} we will present a new FOM-based method to track complex trajectories.
\item{\textbf{Comparing it to other techniques:}} In section \ref{subsec:miqp} we will discuss some of the difficulties we found while trying to compare it to MIQP and we present a modified version of the method to address this problem.
\item{\textbf{Providing a systematic set-up:}} In chapter \ref{sec:modeselection} we will present a systematic protocol with an stochastic approach to choose the modes of the family. TODO: Add riccati equation solving for Qf? add it in improving the method?
\item{\textbf{Showing the system scalability and robustness:}} We wanted to track more complex trajectories, more complex systems, and use models of a simple system to solve a problem that uses a more complex one (TODO: Make sure we got the experiment on time). We will discuss the obtained results in chapter \ref{sec:experiments}.
\end{enumerate} 

In order to achieve this goals, we provided quantitative tools to evaluate the obtained trajectories that will be presented in section \ref{subsec:costfunc} and we implemented a general simulation interface (presented in chapter \ref{sec:sim}) that allowed us to easily formulate and solve general trajectories and hybrid systems.

We plan on submitting the results of this study to 2017 International Symposium on Robotics Research (ISRR) to further spread FOM across the robotics research community.


\section{My Work. TODO: Change name}
\label{sec:work}


\begin{itemize}
\item {State:}
\item {Mode:} Basic modes are S, $U_n$ and $D_n$
\item {Family:}
\item {Control iteration:}
\item {Optimization iteration:}
\end{itemize}


\section{Simulation Tool}
\label{sec:sim}

The simulations of the previous study were mainly designed for an specific choice of modes and a specific trajectory to follow. In this study, we wanted to study general properties of the FOM problem, to generalize the results to more complex pusher-slider systems and trajectories, and to compare our different FOM strategies to solve the MPC problem with other conventional methods as MIQP. In order to satisfy this needs, we decided to implement a general simulation tool. The code is added in appendix \ref{app:code} but the main structure is explained here.

An abstract state interface is implemented to provide the dynamics and cone constraints of the state as well as their linearised versions at a certain objective state space and control input. To define a certain experimental problem, the dynamics function and constraint equations are derived depending on a general state space variable \textbf{x} and controller action \textbf{u} and the necessary state instances are implemented and a mode is then defined by a vector of state handles. An MPC solver class it's also implemented to get an initial condition and an objective trajectory and return a control input. Each MPC solver implementation can include its own member states or modes (as well as other optimization set-up variables) and internally decides how to handle them.

At the same time, an Euler integration simulator is implemented. This simulator uses it's own state classes to describe the dynamics of the pusher slider system. These states are independent from the optimization ones so that discrepancies between model and reality can be simulated (for example the MPC solver can use linearised dynamics or cone constraints while the Euler integrator uses the non-linearised version, which is more similar to the real experimental set-up than using linearised dynamics in both places). The Euler integration class also holds a member MPC solver that returns the control input to be applied. The integrator then computes the value of the state variable derivative $\dot{x}$ and uses a simple quadrature to approximate the next state variable value.

\subsection{Terminal Cost and Riccati Equation}
This term is included in MPC problems to account for the finite horizon. (TODO: Say why, talk about the Riccati to simulate infinite horizon, etc. Talk with frank and alberto). When scoring the final trajectory we are not trying to

\subsection{Global Cost Function}
\label{subsec:costfunc}
During the previous work at MCubelab, there was no systematic and objective way to evaluate the global trajectory resulting from the FOM control law, the exit or failure of an experiment was mainly determined in a subjective binary way. If the pusher slider didn’t show appreciable divergence (at the naked eye) from the planned trajectory it was considered a success, otherwise it was a failure. In a similar fashion if the system was perturbed (by an external interaction), the success lied in recovering the original trajectory in a “reasonable” time determined subjectively. The main goal of the study was just to demonstrate that FOM could effectively control the pusher slider system to track previously planned trajectories, and that all this process could be done online. Because of this proof-of-concept nature, this approach was sufficient.

For this thesis we wanted a more objective way to evaluate how good a control trajectory was and this required a certain notion of metric or score for the global trajectory. We tried to keep coherence between the global and local cost functions, but there were some changes that we deemed necessary. The first one was to remove the $x_N^T Q_{f}^{} x_N^{}$ term of the cost function. As explained in the previous section, this term is used in MPC to account for the cost of the trajectory after the finite horizon end (TODO: Alberto). The second was to remove the R matrix, that penalizes different control inputs between the objective trajectory and the control one. TODO: Alberto. The obtained cost function was $S = \sum\limits_{i = 0}^{N}{x_{i}^{T} Q x_i^{}}$

It is important to note that even if this global cost function or metric is used score how close the obtained trajectory was from the desired one, it is not a direct indicator of the MPC solving performance. Getting the optimal local cost at each iteration of the MPC could incur in a larger optimal cost for the global trajectory. If we are currently at the state $x_i$ and we consider two modes $M_1$ and $M_2$ with local costs $c_1$ and $c_2$, with $c_1 < c_2$ there's no guarantee that executing the first step of the trajectory obtained from mode $M_1$ will translate in a smaller global cost function. It wouldn't be the case even if we used as a global cost function the sum of local costs. Some causes for this effect are exposed and fixed in \ref{subsec:chameleon} but, even after fixing them and considering an ideal world scenario (where the models are completely accurate and there's no sensor noise), the fact that the receding finite horizon adds new information to the problem at every control iteration implies that the optimal state and control pair $(x_{i+1}, u_{i})$ computed at the control iteration i could technically lose their optimal property when the information from control iteration i+1 is introduced.

This property is common in all receding finite horizon problems and there's usually no way to completely solve it. Fortunately, after fixing the mentioned causes, this effect became almost negligible, as will be exposed and discussed in \ref{subsec:simresults}.

\subsection{Chameleon Mode}
\label{subsec:chameleon}

In the previous FOM study there was no guarantee of improvement in the obtained trajectory by considering new modes in a previously used family. It needs to be clarified that, even if in the previous study there was no global score function implemented, this effect could be so big that it could be observed with the naked eye. An illustrative example would be the following. A family containing only the sticking mode $F_1 = S$ would greatly outperform the extended family $F_2 = S \bigcup D_2$. On the other side a further extension $F_3 = F_2 \bigcup D_1$ would outperform the two previous families TODO: ADD PHOTOS OR VIDEOS OF THIS.

The performance decrease experienced in $F_2$ can be explained by the fact that the FOM optimizer can choose optimal trajectories that will never be able to be applied. For example, the optimal control solution at a certain optimization iteration can be to stick for one time step to then slide up for another time step and finally stick for the rest of the finite time horizon prediction. The control law for the controller is to execute the first step of the optimal trajectory, so it the pusher will move without sliding for one time step and then the control law will be recomputed. The problem lies in the fact that no mode starts with a sliding up state, so the trajectory planned in the previous iteration is no longer considered. If the first sliding step of the trajectories obtained from $S$ and $D_2$ are similar there will be no appreciable but the solution of $D_2$ may sometimes rely on the possibility of sliding to follow a more aggressive control trajectory that would have a smaller TODO: FINISH THIS. 

The conclusion these experiences led us to was that, using [TODO: Cite] terminology, there was no sequential improvement property when adding new modes.
Solving the MPC using the FOM approach has similarities with their study: Each of the modes can be considered a heuristic and the strategy of recomputing the control policy at a certain frequency by using the same heuristics can be viewed as a roll-out technique. It's important to note that the concepts of sequential consistency or optimality cannot be applied to our case without some previous modifications, because of the existence of a receding finite time horizon, but for every FOM problem we can formulate an associated Full Horizon Family of Modes (FFOM) as follows: TODO: Define it.

\begin{example} $\boldsymbol{F_1}$ \textbf{is sequentially consistent in it's FFOM version and ideal world assumption(TODO: define it in nomenclature):}

Consider that at a certain control iteration there's a full horizon of N steps to reach the end of the trajectory. Then the $S$ mode will get the optimal path to reach the goal while keeping the sticking contact at all times, obtaining a sequence of position states $\boldsymbol{x^{S}(\diff{x_0})} = (x_1^S, x_2^S, ..., x_N^S)$ and control inputs $\boldsymbol{u^{S}(\diff{x_0})} = (u_0^S, u_1^S, ... u_{N-1}^S)$ for the optimal path from the initial position to the goal. Then, the controller will execute $u_0$ for one control step and the pusher slider system will be in position $x_1$. The only heuristic it will consider in the next optimization step is to stick until reaching the goal, so the optimal trajectory obtained will be $\boldsymbol{x^{S}(\diff{x_1}^S)} = (x_2^S, x_3^S, ..., x_N^S)$ and $\boldsymbol{u^{S}(\diff{x_1}^S)} = (u_1^S, u_2^S, ... u_{N-1}^S)$, being sequentially consistent.
\end{example}
\begin{example} \label{ex:f2notconsistent} \textbf{On the same conditions, $\boldsymbol{F_2}$ is not sequentially consistent nor improving:}

If mode $D_2$ is chosen at a certain time step, giving an optimal trajectory $\boldsymbol{x^{D_2}(\diff{x_0})}$ and control $\boldsymbol{u^{D_2}(\diff{x_0})}$, a mode $D_1$ is required in the family to consider the continuation of the previous optimal trajectory $\boldsymbol{x^{D_1}(\diff{x_1}^{D_2})}$,  $\boldsymbol{u^{D_1}(\diff{x_1}^{D_2})}$ again. As it is not the case, the new optimal trajectory has no relation with the previous one and consistency and improvement cannot be guaranteed.
\end{example}

\begin{remark}
If $\boldsymbol{x^{D_2}(\diff{x_0})} = (x_1^{D_2}, x_2^{D_2}, ..., x_N^{D_2})$ then $\boldsymbol{x^{D_1}(\diff{x_1}^{D_2})} = (x_2^{D_2}, ..., x_N^{D_2})$, the maximal suffix of the first trajectory. (TODO: ADD proof from dynamic programming course or citation)
\end{remark}

\begin{example} \textbf{On the same conditions, $\boldsymbol{F_3}$ is sequentially improving:}

Following the reasoning in example \ref{ex:f2notconsistent}, including the mode $D_1$ in the rollout allows to continue considering optimal trajectories from a previous step of a $D_2$ mode and the $S$ mode considering the ones obtained from a $D_1$ mode.
\end{example}

We considered important to guarantee sequential improvement for the FFOM problem even taking into account that our problem is not in the ideal world (modelling errors and different frequencies for the optimization problem and the controller can lead the state obtained when applying $u_0$ to $\diff{x_0}$ to be different from $x_1$) and that we don't have a full horizon. It was not a sufficient condition to obtain an overall improvement when adding modes to a family, but it was a required one. Another important motivation to try to guarantee it was that the simulation results showed that families that guaranteed improvement in the FFOM problem outperformed the ones that did not.

In order to guarantee it we decided first decided to only use families that would always compare the current possible trajectories to the immediately previous optimal one (in the FFOM problem). The problem with this approach is that for any mode $M = (S_1, ..., S_N)$ you add include in the family you need to include any suffix of it extended to length N $(S_i, ... S_N, S_{N+1}, ... S_{N+i-1})$ and suffixes of the newly added modes would also be required. Given certain mode patterns and adding simple extension tails formed by i - 1 copies of the same mode may keep the number of modes low. For an $S$ mode no extra modes are required and for a $D_n$ one you only need to extend the family with an $S$ mode and all the $D_i$ modes for $i \in (1, n-1)$. Even taking this into account, we considered adopting this strategy constrained the problem of family design more than we wanted to and added more extra nodes that was strictly necessary, so we decided to change from static families to dynamical ones instead.

The dynamical adjustment we added consisted in including a dynamic mode that would force the algorithm into considering the previously chosen path to guarantee sequential improvement. This mode, that we called chameleon mode, simply includes the maximal suffix [TODO: add definition] of the previously selected mode extended with a single sticking state. TODO: Add something.

The chameleon mode functionality was added in the simulation interface and it's performance was tested in simulation. As we expected, there was a great improvement in families that didn't have a sequentially improving structure without it. TODO: Add figures with comparison and some comment.

With the chameleon mode, an improvement when adding modes could be observed to a certain extent, as will be discussed in \ref{subsec:learningsimple}.

Given the good properties of the chameleon mode, we decided to include it in the FOM formulation. From now on, unless specified otherwise, we will use FOM to refer to the family of modes with chameleon mode implementation.

\subsection{MIQP and Clustered MIQP}
\label{subsec:miqp}

We wanted to compare the solutions provided by FOM with traditional MPC solving techniques. In order to do that we decided to implement the MIQP technique using the commercial solver Gurobi. This solver was already used in Hogan's work in a force-based formulation prior to the final velocity-based one (TODO: Cite) and it was able to run in an average of 3Hz. It should be mentioned that the execution time at every control iteration could range between few milliseconds to 10 seconds, because MIP algorithms depend on the geometry of the problem.
This is not a problem for simulation, but in a real control environment the frequency at which you can update the controller cannot be higher that the frequency at which you can solve the worst case scenarios in the optimization problem. Otherwise the controller may try to update the control input before it is computed by the optimizer. There may be solutions to this effect in other formulations, but for the one used in Hogan's work an MIQP controller could not be run at more than 0.1Hz.

We were expecting a similar behaviour in the velocity-based formulation, but it took several orders of magnitude more to solve. In order to simulate 500 control iterations (a trajectory of 5 seconds controlled at 100Hz) with a finite horizon of 7 steps it took around 6 hours of execution time. We were expecting to have an "ideal" MIQP result to compare FOM with and try to get similar results with FOM with less computational effort, but the results obtained with 7 steps of the MIQP were worse than the FOM and we couldn't afford to increase the horizon further.

We looked at techniques to speed up the MIQP execution (TODO: Add where did I find the information) and we found out that our current big-M formulation worked better (both in execution time and in quality of the obtained trajectory) than the indicator constraints formulations as smaller the M parameter was. As explained in \ref{subsec:intro_miqp} each M parameter used in a scalar equality $Ax \leq b + M (1 - z)$ must be greater than $\max\limits_{x \in D}{(Ax - b)}$, for a the x variable domain D, so that the inequality is rendered redundant when z = 0. We implemented a version of MIQP that given a vectorial constraint as the ones used in Hogan's formulation $A\vec{x} \leq \vec{b} + M (1 - z) \cdot (1, ..., 1)^T$ replaces the arbitrary constant M by a vector $\vec{M}$ where each component's value is $M_i = \max\limits_{x_i \in D}{(A_i x_i - b_i)}$
. We were not able to improve the execution time, but we were able to obtain better results for the MIQP. TODO: add images showing the improvement.

In order to compare FOM to other methods using a longer time horizon, we implemented another MPC solving method based on MIQP that we called Clustered MIQP. Given a standard MPC formulation, this method works in the following way:

\begin{itemize}
\item Consider a clustering factor c, a finite horizon $ N c$ and a set of h hybrid states $ \{ H_1, ..., H_h \}$.
\item The finite horizon is then divided into N clusters of c steps: $(C_1, ..., C_N)$.
\item The state space is extended with 3 binary variables per step $(s_i, d_i, u_i)$ for a total of 3N variables.
\item Each variable $s_i$ $d_i$ and $u_i$ is associated with the hybrid state of the cluster $C_i$ (sticking, sliding down and sliding up respectively).
\item The constraint $s_i + d_i + u_i = 1$ is enforced 
\item The problem constraints are formulated with big M notation or indicator constraints and the problem is fed into an MIQP solver.
\end{itemize}

This way we can reduce the dimensionality of the optimization problem by keeping the same state during all the steps of the same cluster. The execution time of this new problem is similar to the execution time of a MIQP problem of N steps but the controller inputs can be different at each of the $N c$ steps.

We intended to use this method to approximate MIQP problems with long horizons (around 35 steps) by using few clusters (around 5) and we expected to obtain a better trajectory than the ones obtained with FOM. As will be discussed later, the results of this MIQP approximation were worse than the results obtained in FOM with a good choice of few modes.

\subsection{Simulation Conclusions (TODO)}
\label{subsec:simresults}

We were able to compare both MIQP and FOM with a small finite horizon of 5 steps and we were able to observe the expected behaviour: Some families gave poor results when compared to MIQP, but the result could be iteratively improved by adding new modes. The obtained trajectories did not strictly follow an increasing sequence, but 

\section{Mode Selection Protocol}
\label{sec:modeselection}
In the previous sections it is explained how we addressed the sequential properties of the FOM formulation and provided a metric and other MPC solvers to compare FOM to. At this point of our study, we were able to adress our next objective: To optimize the selection of the family modes for the FOM formulation.

We considered multiple optimization criteria and relations to study and we had to adopt a set of assumptions to be able to obtain some initial results. For our first study, we tried to learn the modes to follow a single fixed straight line trajectory. Each optimization problem would be parametrized by a fixed finite time horizon (characterized by a number of steps N) and a fixed number of modes K a family could contain. The initial condition $\diff{x_0}$ was assumed to be a random variable following a certain probability distribution. The cost of the control trajectory C was thus also a random variable of unknown distribution parametrized by the optimization problem parameters. Specifically given a set of K modes $\{m_1, ..., m_k\}$, $C \sim D(m_1, ..., m_k)$ for a given distribution law D. Ideally, we would like to choose the the set of modes that minimizes $\mean{C}$.

We made three main assumptions:
\begin{assumption} \label{ass:localcost}
\textbf{Local Cost Sufficiency:} Optimizing the modes to reduce the local costs obtained while solving each MPC iteration would be sufficient to obtain a low global cost.
\end{assumption}
\begin{assumption} \label{ass:localinv}
\textbf{Locally Invariant Trajectories TODO: Check it's a good name:} Given a local inertial frame F centered on the slider center of mass moving and rotating with it and given an infinite objective trajectory $C(t)$ for $t \in (-\infty, \infty)$, then the $C(t)$ would be time invariant in frame F (for example an infinite circle loop or an infinite straight line).
\end{assumption}
\begin{assumption} \label{ass:singlestep}
\textbf{Single Step Sufficiency:} A single control iteration of the trajectory tracking problem was sufficient to learn which modes where optimal, if the appropriate distribution for $\diff{x_0}$ was chosen.
\end{assumption} 

The motivation and justification for these assumptions will be explained in \ref{subsec:structure}, as well as some other implicit assumptions contained in it. In \ref{sec:todo} we will discuss some of the studies we would have conducted and how we would have modified these assumptions if we disposed of more time.

\subsection{Structure Study And Systematic Approach}
\label{subsec:structure}
Given a finite horizon of N steps and H states and considering M the set of possible modes obtained by concatenating N of these hybrid states, then $|M| = H^N$. Let the set of possible families containing K of these modes be F. Then $|F| = \binom {|M|}{K} = \frac{H^{N}!}{K!(H^N-K)!}$. Even with a low number of states this number grows quickly as N and K increase.

If we tried to directly tackle the optimization problem without any assumption or knowledge of its structure, we would need to solve several trajectory tracking problems for each of these possible families, because the families could be correlated between themselves. A simple example that will probably clarify this effect is that a $D_1$ or $U_1$ family will probably perform poorly alone because if the pusher slides away from the center of the slider face it will never be able to go back to it and control near the edges of the face is less versatile than on the middle (for example, while steeper turns can be taken in the edges of the face, changing rotation direction is much more difficult, at some point not even possible without sliding to the other side).

Solving many different initial conditions for all possible mode combinations with a long enough finite horizon would take too much time even to learn the optimal modes to track a single trajectory. We didn't want to use any simplification that would be specific to our pusher-slider system, because we wanted to provide systematic tools to use the FOM approach in general problems the control community may face. This led us to use a set of general assumptions to simplify the problem and devise a systematic protocol to study the structure of each problem and use different optimization/learning techniques depending on the structure.

We mentioned the local cost sufficiency assumption again here, but this assumption was already taken when deciding to use a finite horizon MPC formulation to control the pusher-slider system. The mode selection will determine the performance of our local FOM solver and the relation between local MPC performance and overall performance and the validity of this assumption were already discussed in \ref{subsec:costfunc} and \ref{subsec:simresults}.

The locally invariant trajectories assumption motivation will be explained later in \ref{subsec:dynfom} (TODO: Make sure I don't forget to explain it)

The main motivation for the single step sufficiency assumption has to be understood under the previous assumption. The locally invariant property implies that the only difference between an MPC step and another is their initial condition random variable $\diff{x_i}$. Consider a distribution law X that focusses more on covering a wide range of possible values of any $\diff{x_i}$ rather than on their frequency as, for example an appropriate uniform distribution. We can then optimize the modes to reduce the expected value of $C(\diff{x_0})$ of a single controlling step tracking problem, where $\diff{x_0} \sim X$. Optimizing the mode selection this will simultaneously reduce the local costs expected value for all $\diff{x_i}$ implicitly assuming that each of these values are equally distributed.

This implicit assumption is clearly false. The fact that some trajectories converge is already a counterexample of it. We consider assuming a uniform to describe all $\diff{x_i}$ is a conservative approach, as will consider rare big disturbances (that can usually only happen with an external perturbation or a bad initial position) to be equally likely to appear at any time step as small disturbances characteristic of proper control around the objective trajectory. We expected this approach would be sufficient to provide a mode selections that would be robust enough to track a wide range of trajectories and initial conditions, but that could fail to capture some feedback relations between modes. This will be further discussed in \ref{sec:todo}.

These assumptions result in a great cost reduction and simplification of the learning algorithm because the optimization problem can be now be easily written as a function of $\diff{x_0}$:
\begin{corollary} \textbf{The optimization problem to select the FOM modes can be expressed as:} \label{cor:costfunction} 
 $$\min\limits_{m_1, ..., m_K \in M}\mathbb{E}(C(\diff{x_0}; m_1, ..., m_K)) = \min\limits_{m_1, ..., m_K \in M}\mathbb{E}(\min\limits_{i = 0}^{K}{C_i(\diff{x_0})})$$
 
\end{corollary}
Where $C_i$ is the cost associated to mode $m_i$ in the MPC optimization. This further implies that solving the MPC problem for all the possible sets of K modes is no longer necessary. If $C_i(\diff{x_0})$ is computed for every mode $m_i \in M$, then $C(\diff{x_0}; m_1, ..., m_K)$ is defined for any set of K modes by corollary \ref{cor:costfunction}.

This algorithmic cost reduction was not enough to face the full learning problem. We no longer had to study the full space of families F, but we still had no alternative to explore the full space of modes M, even if we just try to obtain a local minimum. Many local optimization techniques can avoid considering the full space it by relying on the structure of the optimization problem. For example, when the parameters are on a continuous metric space, the concept of gradient arises and methods as gradient descent can be used. Some structures can also be used to guarantee stronger properties as in convex problems, were local solutions are also global.

Considering all these aspects, we devised a systematic approach to study the structure of a general MPC problem and optimization techniques. We then provided a set of optimization techniques to be applied to obtain an approximate solution for the optimization problem. It has to be noted that the studies and techniques we will present are not proofs of a certain structure and thus, the techniques do not guarantee any kind of optimal solution. We considered proving any of these properties in an specific problem to be an object of research study in itself and it escaped the goal of this thesis, as we wanted to provide general simple tools to expand the FOM usage in the manipulation community.

TODO: Maybe get rid of all this last part, never been a fan of it.
\begin{itemize}
\item {Finite horizon increase sensibility:}
\item {Mode metric study:} In section (TODO:reference it) we provide and discuss a mode metric. We wanted to study if there was a continuity-like property (if we bound the bound distance between modes we can bound to some extend the distance of their costs). Having this property would allow to 
\item {Cost distribution study:} If all have the same distribution, fewer samples would be required, or at least we could have boundaries on the error.
\end{itemize}

\subsection{Simple trajectory learning}
\label{subsec:learningsimple}
For our initial study, we tried to learn the best modes for single locally invariant trajectories. As proposed in the previous section, we started by obtaining 1000 samples of $\diff{x}$ from a uniform distribution in (TODO: finish specifications) and solving for each initial condition the tracking problem for all the modes with a small finite horizon of 5 steps. This implied 243 possible modes, for which 1000 initial conditions were used, for a total of 243.000 single step tracking problems. The cost functions $C_i(\diff{x_j})$ were computed for each mode-initial condition pair.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=16cm]{surf_cost_pre_std.jpg}
\end{center}
\caption{\label{surf_pre} \small Example surface plot of $C_i(\diff{x_j})$ for 50 initial condition samples, a finite horizon of 5 steps, and a line as objective trajectory.}
\end{figure}

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=16cm]{surf_cost_after_std.jpg}
\caption{\label{surf_post} \small Example surface plot of $\hat{C_i}(\diff{x_j})$ for 50 initial condition samples, a finite horizon of 5 steps, and a line as objective trajectory}
\end{center}
\end{figure}

We were concerned that the initial condition had a lot of effect in the overall cost of the mode-condition pair, because higher or smaller disturbances would lead to higher or smaller overall cost respectively (TODO: add histograms of a couple experiments for fixed initial condition to capture that), independently of the mode chosen. We wanted to give more importance to the fact that mode had a cost c for a certain initial condition if the rest of the modes resulted in higher costs than if they resulted in similar or even lower costs (TODO: add examples?). In a sense, we wanted the obtained costs to be comparable between experiments, so we decided to standardize the results obtained within each experiment. That is, given $C_i(\diff{x_j})$ we computed $\mean{C_i} = \dfrac{\sum\limits_{j = 0}^{243}{C_i(\diff{x_j})}}{243}$ and $\sigma_i = \sqrt{\sum\limits_{j=0}^{243}{\dfrac{(C_i(\diff{x_j}) - \diff{C_i})^2} {242}}}$ and we used them to compute $\hat{C_i}(\diff{x_j}) = \dfrac{C_i(\diff{x_j} - \diff{C_i}}{\sigma_i}$. The results can be observed in figure \ref{hist_after_st}.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=6cm]{standarization_single_mode.png}
\caption{\label{hist_after_st} \small Histogram of $\hat{C_i}(\diff{x_j})$ over all the initial conditions with a fixed mode.}
\end{center}
\end{figure}
(TODO: Ensure the images are on the same mode and change labels so that it makes more sense)

We were not able to determine a clear probabilistic distribution for $\hat{C_i}(\diff{x_j})$ (in figure \ref{his_after_st} an example histogram of the standardized costs for a fixed mode is shown. As can be seen, no clear distribution can be observed) (TODO: use a histogram for the same experiment) so we decided to minimize the statistical mean for each distribution $ \bar{C}(\diff{x}; m_1, ..., m_K) = \frac{\sum\limits_{j=0}^{N}{\min\limits_{i = 0}^{K}{C_i(\diff{x_j})}}}{N}$. For that purpose we used a dynamic programming algorithm to choose the K modes that minimized $\min\limits_{m_1, ..., m_K \in M}\mathbb{E}(\min\limits_{i = 0}^{K}{\hat{C_i}(\diff{x})})$. As you can see in figure (TODO: add figure) this properly captures relations between modes. The algorithm can choose modes that, even if they would not provide small means by themselves, can greatly increase their performance by associating with modes that specialize in initial conditions where they fail to give a good result.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=16cm]{imagesc_after_std.jpg}
\caption{\label{imagesc_post} \small Example imagesc plot of $\hat{C_i}(\diff{x_j})$ for 50 initial condition samples, a finite horizon of 5 steps, and a line as objective trajectory}
\end{center}
\end{figure}

We then proceeded to follow the strategies cited in the previous section. We successfully computed an approximation of the best modes for a 35 step length horizon by iteratively selecting the 5 best modes for $5 \cdot i$ steps for $i \in [1, 6]$ and extending them to $5 \cdot (i + 1)$ steps by adding all the possible tails to the previous best modes. Experimental results showed us that state differences were more important when they occurred near the beginning of the mode than the end. This motivated us to compare the results obtained by following the full iterative procedure and the results obtained by computing the best modes for a short horizon and extending it with a long simple tail.

(TODO: add pictures of the comparisons)

The results obtained showed that adding a simple sticking tail to the best K modes chosen for a short trajectory was enough to successfully track a trajectory. (TODO: study with different tail)

(TODO: add graph with this)

Experimental results also showed that the improvement obtained by adding new modes decreased exponentially (TODO: check the proper name for this) before reaching the optimal value, as can be seen in figure \ref{cost_over_modes}. After a few modes, the results are highly stable and the changes in the cost function are small.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=16cm]{cost_over_modes.jpg}
\caption{\label{cost_over_modes} \small Example imagesc plot of $\hat{C_i}(\diff{x_j})$ for 50 initial condition samples, a finite horizon of 5 steps, and a line as objective trajectory}
\end{center}
\end{figure}

Finally, we tested the robustness of the method by tracking circular trajectories with the best modes obtained for a straight line. The behaviour was completely different if the initial condition were outside or inside of the tracking circle. As can be seen in (TODO) few modes were sufficient to properly track trajectories with high external disturbances. On the other hand, small internal disturbances could lead to solutions with a steady state error that ended tracking a circle internal to the one determined by the objective trajectory if few modes were used. Progressively adding modes steadily reduced the effect.

(TODO: add graph)

TODO: Circle with circle modes

\subsection{Dynamic FOM}
\label{subsec:dynfom}
We wanted to be able to track more complex trajectories that did not require the locally invariant property. The previous experimental results and motivated us to develop a variation of the FOM method. This consisted in training the FOM method on several base simple locally invariant trajectories that could describe most of the situations the tracking problem would face and then the best k modes were learned for every base trajectory. For the simple contact pusher-slider system this consisted in a straight line and several circular trajectories of different curvatures.

\begin{remark}
We will use the common mathematical notation for the curvature sign. That is, a positive curvature means that the unit tangent vector rotates counter-clockwise as a function of the parameter along the curve.
\end{remark}

The chameleon mode brought the idea of having dynamic modes in a family and, in a similar way, the dynamic FOM brough the concept of having dynamic families in a (TODO: a or an?) MPC problem. At each control step, the best base trajectory to describe the local geometry of the objective trajectory was chosen. Then, the best k modes for that base trajectory were used (together with the chameleon mode) to solve the MPC problem on that instant. For the pusher-slider system mentioned before, the base trajectory was chosen by numerically computing the curvature of the objective trajectory at every control step and using the base trajectory with the most similar curvature value.

The results obtained in the previous section showed that the initial part of the modes generally held more importance than the last part. This motivated us to optimize the modes to solve simply the local approximation of the general trajectory. We expected that a sufficient control frequency would allow the system to properly track the general trajectory even if its local properties undertook steep temporal changes.

The main motivation for this method was to reduce the online cost of the FOM approach at the expense of more offline computations. We expected that choosing the best family dynamically would reduce the number of modes per family required to obtain good results in a broad range of trajectories. Having n modes in a family implies solving n convex optimization problems at every control step. The cost of solving each optimization problem may vary, so we cannot directly assume that the overall execution time of the algorithm is proportional to the number of modes in the family, but experimental results show so for our specific problem (TODO: add graph). Even without these experimental results, we still consider important for the control problems to reduce the online computational cost of the algorithm.

In the next section we will present the simulation results obtained while tracking a complex trajectory by using the dynamic FOM and a static FOM learned to track a straight line.

\section{Experiments}
\label{sec:experiments}

\section{Conclusion and Discussion}
\label{sec:conclusion}
\begin{enumerate}
\item{MIQP} takes too long, it's execution time depends on the geometry of the problem (different time at each iteration). The results may depend on the implementation chosen. If you don't have a software that allows you to tune everything it is too complicated to implement. Clustered MIQP is no better. We should try MIQP with tail (we expect good results but bad time)
\end{enumerate}

\section{Things to do if we had time}
\label{sec:todo}
\begin{itemize}
\item Study modes correlation
\item Study optimal mode dependency wrt initial condition.
\item Change the uniformly distributed initial conditions by normally distributed.
\end{itemize}

\section{TODO stuff}
I have to talk about close loops and about manipulation. then first approach to the family of modes. Then sequential consistency, improvement.

This is an example of a document using the TFG-GM.cls document class. The TFG-GM.cls document class is a modification of the Reports@SCM class with minor differences (cover page, title colors and format for references) to facilitate the submission of your work to the journal Reports@SCM.

If your plan is submitting your work to the \textbf{journal Reports@SCM}, please note that:
\begin{itemize}
	\item The length of the core of the document should not exceed 10 pages, see the Reports@SCM web page for details.
	\item Further developments, explanations, codes or results are expected to be also included in this document as appendixes.
	\item You should not add any extra packages unless you consider it very necessary. See Section \ref{packages} to see which standard packages  are considered by default. 
\end{itemize} 

If you do not plan submitting your work to the journal  Reports@SCM, you can use this document as an example. \textbf{Using this template is not mandatory}.
 
In any case, \textbf{you must use the template for the main cover page} \texttt{coverMAMMEmasterThesis.doc} as explained in section \ref{sec:coverPage}.


\section{Adding the MAMME cover page to your document}
\label{sec:coverPage}
  
Regardless of the structure of the document of your TFG, you have to use the template \texttt{coverTFG-GM.doc} for the cover page. You can follow the fowolling steps:
\begin{itemize}
	\item Generate a pdf file with the document of your TFG, following or not this template
	\item Modify the document \texttt{coverTFG-GM.doc} with the data of your thesis and generate a pdf with two pages (cover and blank page)
	\item Use Adobe or other sofware to join (combine or merge) the two pdf files in one pdf file. 
\end{itemize}
 
\section{Environments}

In this section you can see examples of different environments.

\subsection{Theorem-like environments}

\begin{theorem} \label{th:example}
This is an example of a theorem, numbered with the section number.
\end{theorem}

\begin{proposition}
This is a proposition, numbered the same way. You can reference theorems and propositions using the labels, see for instance Theorem \ref{th:example}.
\end{proposition}

\begin{lemma}
This is a lemma, numbered the same way.
\end{lemma}

\begin{corollary}
This is a corollary, numbered the same way.
\end{corollary}

\begin{conjecture}
This is a conjecture, numbered the same way.
\end{conjecture}

If you need any other environment, you can add it to the preamble following the examples.

%%%%
\subsection{Definition-like environments}

\begin{definition}
This is a definition. Notice that the style is different than in theorems.
\end{definition}

\begin{example}
This is an example. Same style as definitions.
\end{example}

%%%%
\subsection{Remark-like environments}

\begin{remark}
This is a remark. 
\end{remark}

\begin{remarknonumber}
This is a remark with no numbered label. You may create other environments with no numbered label by copying from this example.
\end{remarknonumber}

%%%%%
\section{Figures}

Please include figures using the graphics package uploaded.  Fancy options can be found for example in  \begin{verbatim} http://www.kwasan.kyoto-u.ac.jp/solarb6/usinggraphicx.pdf \end{verbatim}

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=6cm]{samplefigure.pdf}
\end{center}
\caption{\label{sample figure} \small The caption of this figure is ``Newton's method of a cubic polynomial".}
\end{figure}

%%%%%%%%
\section{Mathematics and packages} \label{packages}

By default, the following packages are uploaded:
\begin{enumerate}[\bf (1)]
\item {\tt enumerate:} It allows you to make list with specific somehow arbitrary labels, like this one.
\item {\tt amsthm:} To make evironments with different styles.
\item {\tt amsmath,amssymb,amsfonts:} Multiple mathematics symbols and fonts.
\item {\tt graphicx:} To include figures in a simple and intuitive way.
\item {\tt amscd:} To make commutative diagram with horizontal and vertical arrows. See below.
\item {\tt xy:} To make really fancy commutative arrows. See below.
\item {\tt booktabs:} To make fancy tables.
\end{enumerate}
You may add other standard packages if you need them but try to avoid it if at all possible.

If you need to use them, you will find information about these packages in the usual internet places. 

Here are examples of two commutative diagrams, one made with the package amscd and the other one with xy.

\[
\begin{CD}
A @>g>> B\\
@VV\pi V @VV\pi V\\
X @>f>> Y
\end{CD}
\]


\section{Bibliography}

You may include your references by hand using {\tt the bibliography} (see an example below) or, alternatively, you may use a .bib file and use BibTeX. In any case, we ask you to use a reasonable {\bf consistent} format for all your references. Our recommendation is using BibTex with the style   "plain" or "amsalpha".

%\newpage

\bibliography{fom_bib}{}
\bibliographystyle{plain}

%______________________________________________________________
\appendix
\vfill\newpage \section{Simulation Interface Code}
\label{app:code}
You can include here an appendix with details that can not be included in the core of the document. You should reference the sections in this appendix in the core document.
\vfill\newpage \section{Title of the appendix}
Second appendix.

\end{document}


